from fastapi import APIRouter, Depends, HTTPException, Request, logger, status 
from grpc import Status
from sqlalchemy.orm import Session
from typing import Dict, AsyncGenerator, List, Optional
import asyncio
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
from services.chat_service import chat_with_ai
from crud.chat import get_chat_history
from db.session import SessionLocal
from models.operation_history import OperationHistory
from schemas.chat import ChatHistoryResponse, ChatRequest
import logging
# import threading
# active_lock = threading.Lock()  # æ·»åŠ åœ¨è·¯ç”±æ–‡ä»¶é¡¶éƒ¨
# å°†çº¿ç¨‹é”æ”¹ä¸ºå¼‚æ­¥é”
import asyncio
from contextlib import asynccontextmanager

# åˆå§‹åŒ–å¼‚æ­¥é”
active_lock = asyncio.Lock()

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

chat_router = APIRouter(
    prefix="/chat",
    tags=["Chat AI"]
)
active_generators: Dict[str, asyncio.Queue] = {}

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@asynccontextmanager
async def create_abort_channel(abort_key: str):
    queue = asyncio.Queue(maxsize=1)
    
    # ä½¿ç”¨å¼‚æ­¥é”çš„æ­£ç¡®æ–¹å¼
    async with active_lock:
        # æ¸…ç†æ—§è¯·æ±‚
        if abort_key in active_generators:
            old_queue = active_generators[abort_key]
            try:
                await old_queue.put("ABORT")
                logger.info(f"â™»ï¸ ç»ˆæ­¢æ—§è¯·æ±‚: {abort_key}")
            except Exception as e:
                logger.error(f"æ—§è¯·æ±‚ç»ˆæ­¢å¤±è´¥: {str(e)}")
        
        # æ³¨å†Œæ–°é˜Ÿåˆ—
        active_generators[abort_key] = queue
        logger.info(f"ğŸ“ æ³¨å†Œæ–°é˜Ÿåˆ—: {abort_key}")

    try:
        yield queue
    finally:
        async with active_lock:
            if abort_key in active_generators and active_generators[abort_key] is queue:
                del active_generators[abort_key]
                logger.info(f"ğŸ§¹ æ¸…ç†é˜Ÿåˆ—: {abort_key}")

# ä¿®æ”¹åçš„ç»ˆæ­¢ç«¯ç‚¹
@chat_router.post("/abort/{abort_key}",  status_code=status.HTTP_202_ACCEPTED)
async def abort_chat(abort_key: str):
    logger.info(f"ğŸ”´ æ”¶åˆ°ç»ˆæ­¢è¯·æ±‚: {abort_key}")
    logger.info(f"å½“å‰æ´»è·ƒé˜Ÿåˆ—: {list(active_generators.keys())}")
    async with active_lock:  # æ­£ç¡®ä½¿ç”¨å¼‚æ­¥é”
        logger.info(f"å½“å‰æ´»è·ƒé˜Ÿåˆ—: {list(active_generators.keys())}")
        if abort_key not in active_generators:
            logger.warning(f"âŒ æ“ä½œä¸å­˜åœ¨: {abort_key}")
            return {"status": "not_found"}
        
        queue = active_generators[abort_key]

    try:
        await queue.put("ABORT")
        logger.info(f"âœ… ç»ˆæ­¢ä¿¡å·å·²å‘é€: {abort_key}")
        return {"status": "aborted"}
    except Exception as e:
        logger.error(f"ç»ˆæ­¢å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@chat_router.post("/")
async def chat_endpoint(request: Request, chat_request: ChatRequest, db: Session = Depends(get_db)):
    operation_id = chat_request.operation_id
    prompt_type=chat_request.prompt_type
    abort_key = normalize_prompt_key(operation_id, prompt_type)
    logger.info(f"ğŸ¬ æ”¶åˆ°æ–°çš„èŠå¤©è¯·æ±‚: operation_id={operation_id}, prompt={chat_request.prompt_type}")
    async def response_generator():
        async with create_abort_channel(abort_key) as abort_queue: # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨,å¹¶ä¸”å¯ä»¥å¤šåŠŸèƒ½åŒæ­¥è¿è¡Œ
            try:
                async for chunk in chat_with_ai(
                    db=db,
                    request=request,
                    operation_id=operation_id,
                    message=chat_request.message,
                    prompt_type=chat_request.prompt_type,
                    abort_queue=abort_queue
                ):
                    yield chunk
            except asyncio.CancelledError:
                logger.warning(f"ğŸ¬ æ“ä½œ {operation_id} è¢«å–æ¶ˆ")
                yield "event: error\ndata: è¯·æ±‚å·²å–æ¶ˆ\n\n"
            except Exception as e:
                logger.error(f"ç”Ÿæˆå™¨å¼‚å¸¸: {str(e)}")
                yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        response_generator(),
        media_type="text/event-stream",
        headers={"X-Accel-Buffering": "no"}
    )

@chat_router.get("/history/{operation_id}", response_model=List[ChatHistoryResponse])
def get_history(operation_id: int, db: Session = Depends(get_db)):
    history = get_chat_history(db, operation_id)
    return history

@chat_router.get("/auto/{operation_id}")
async def auto_patient_basic_analysis(request: Request, operation_id: int, db: Session = Depends(get_db)):
    operation = db.query(OperationHistory).filter(OperationHistory.operation_id == operation_id).first()
    prompt_type="patient_basic_analysis"
    abort_key = normalize_prompt_key(operation_id, prompt_type)
    if not operation:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°å¯¹åº”çš„æ€¥æ•‘è®°å½•")

    if not operation.patient_id:
        raise HTTPException(status_code=400, detail="æœªå½•å…¥æ‚£è€…ä¿¡æ¯ï¼Œæ— æ³•ç”ŸæˆåŸºç¡€åˆ†æ")

    async def response_generator():
        async with create_abort_channel(abort_key) as abort_queue:
            try:
                async for chunk in chat_with_ai(
                    db=db,
                    request=request,
                    operation_id=operation_id,
                    message="è‡ªåŠ¨åˆ†ææ‚£è€…åŸºç¡€ä¿¡æ¯",
                    prompt_type="patient_basic_analysis",
                    abort_queue=abort_queue
                ):
                    yield chunk
            except asyncio.CancelledError:
                logger.warning(f"ğŸ¬ æ“ä½œ {operation_id} è¢«å–æ¶ˆ")
                yield "event: error\ndata: è¯·æ±‚å·²å–æ¶ˆ\n\n"
            except Exception as e:
                logger.error(f"ç”Ÿæˆå™¨å¼‚å¸¸: {str(e)}")
                yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        response_generator(),
        media_type="text/event-stream",
        headers={"X-Accel-Buffering": "no"}
    )

# æ‹†åˆ†apiï¼Œä»¥å®ç°åŒæ­¥è¿è¡Œå¤šä¸ªai request
# 1. form section è·å¾—ä¸»è¯‰~æœ€ç»ˆç»“æœ
@chat_router.post("/optimize_full_entry")
async def chat_optimize_full_entry(request: Request, chat_request: ChatRequest, db: Session = Depends(get_db)):
    operation_id = chat_request.operation_id
    prompt_type=chat_request.prompt_type
    abort_key = normalize_prompt_key(operation_id, prompt_type)
    logger.info(f"ğŸ¬ æ”¶åˆ°æ–°çš„èŠå¤©è¯·æ±‚: operation_id={operation_id}, prompt={chat_request.prompt_type}")
    async def response_generator():
        async with create_abort_channel(abort_key) as abort_queue:
            try:
                async for chunk in chat_with_ai(
                    db=db,
                    request=request,
                    operation_id=operation_id,
                    message=chat_request.message,
                    prompt_type=chat_request.prompt_type,
                    abort_queue=abort_queue
                ):
                    yield chunk
            except asyncio.CancelledError:
                logger.warning(f"ğŸ¬ æ“ä½œ {operation_id} è¢«å–æ¶ˆ")
                yield "event: error\ndata: è¯·æ±‚å·²å–æ¶ˆ\n\n"
            except Exception as e:
                logger.error(f"ç”Ÿæˆå™¨å¼‚å¸¸: {str(e)}")
                yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        response_generator(),
        media_type="text/event-stream",
        headers={"X-Accel-Buffering": "no"}
    )
# 2. smart advice æ ¹æ®æœ€åç»“æœï¼Œç»™å‡ºå»ºè®®
@chat_router.post("/patient_attention_suggestion")
async def chat_patient_attention_suggestion(request: Request, chat_request: ChatRequest, db: Session = Depends(get_db)):
    operation_id = chat_request.operation_id
    prompt_type=chat_request.prompt_type
    abort_key = normalize_prompt_key(operation_id, prompt_type)
    logger.info(f"ğŸ¬ æ”¶åˆ°æ–°çš„èŠå¤©è¯·æ±‚: operation_id={operation_id}, prompt={chat_request.prompt_type}")
    async def response_generator():
        async with create_abort_channel(abort_key) as abort_queue:
            try:
                async for chunk in chat_with_ai(
                    db=db,
                    request=request,
                    operation_id=operation_id,
                    message=chat_request.message,
                    prompt_type=chat_request.prompt_type,
                    abort_queue=abort_queue
                ):
                    yield chunk
            except asyncio.CancelledError:
                logger.warning(f"ğŸ¬ æ“ä½œ {operation_id} è¢«å–æ¶ˆ")
                yield "event: error\ndata: è¯·æ±‚å·²å–æ¶ˆ\n\n"
            except Exception as e:
                logger.error(f"ç”Ÿæˆå™¨å¼‚å¸¸: {str(e)}")
                yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        response_generator(),
        media_type="text/event-stream",
        headers={"X-Accel-Buffering": "no"}
    )

# 3. è¯äº‘
@chat_router.post("/chat_keyword_extraction")
async def chat_chat_keyword_extraction(request: Request, chat_request: ChatRequest, db: Session = Depends(get_db)):
    operation_id = chat_request.operation_id
    prompt_type=chat_request.prompt_type
    abort_key = normalize_prompt_key(operation_id, prompt_type)
    logger.info(f"ğŸ¬ æ”¶åˆ°æ–°çš„èŠå¤©è¯·æ±‚: operation_id={operation_id}, prompt={chat_request.prompt_type}")
    async def response_generator():
        async with create_abort_channel(abort_key) as abort_queue:
            try:
                async for chunk in chat_with_ai(
                    db=db,
                    request=request,
                    operation_id=operation_id,
                    message=chat_request.message,
                    prompt_type=chat_request.prompt_type,
                    abort_queue=abort_queue
                ):
                    yield chunk
            except asyncio.CancelledError:
                logger.warning(f"ğŸ¬ æ“ä½œ {operation_id} è¢«å–æ¶ˆ")
                yield "event: error\ndata: è¯·æ±‚å·²å–æ¶ˆ\n\n"
            except Exception as e:
                logger.error(f"ç”Ÿæˆå™¨å¼‚å¸¸: {str(e)}")
                yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        response_generator(),
        media_type="text/event-stream",
        headers={"X-Accel-Buffering": "no"}
    )
# 4. çŸ›ç›¾é¡¹åˆ†æ
@chat_router.post("/chat_consistency_check")
async def chat_chat_consistency_check(request: Request, chat_request: ChatRequest, db: Session = Depends(get_db)):
    operation_id = chat_request.operation_id
    prompt_type=chat_request.prompt_type
    abort_key = normalize_prompt_key(operation_id, prompt_type)
    logger.info(f"ğŸ¬ æ”¶åˆ°æ–°çš„èŠå¤©è¯·æ±‚: operation_id={operation_id}, prompt={chat_request.prompt_type}")
    async def response_generator():
        async with create_abort_channel(abort_key) as abort_queue:
            try:
                async for chunk in chat_with_ai(
                    db=db,
                    request=request,
                    operation_id=operation_id,
                    message=chat_request.message,
                    prompt_type=chat_request.prompt_type,
                    abort_queue=abort_queue
                ):
                    yield chunk
            except asyncio.CancelledError:
                logger.warning(f"ğŸ¬ æ“ä½œ {operation_id} è¢«å–æ¶ˆ")
                yield "event: error\ndata: è¯·æ±‚å·²å–æ¶ˆ\n\n"
            except Exception as e:
                logger.error(f"ç”Ÿæˆå™¨å¼‚å¸¸: {str(e)}")
                yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        response_generator(),
        media_type="text/event-stream",
        headers={"X-Accel-Buffering": "no"}
    )


def normalize_prompt_key(operation_id: int, prompt_type) -> str:
    """
    ç»Ÿä¸€ç”Ÿæˆä»»åŠ¡å”¯ä¸€æ ‡è¯†ï¼ˆé¿å…å‰ç«¯ä¼ æ¥ tuple æˆ–å…¶ä»–ç±»å‹ï¼‰
    """
    # å¦‚æœæ˜¯ tupleï¼Œè½¬ä¸º str
    if isinstance(prompt_type, tuple):
        prompt_type = prompt_type[0]

    # å»é™¤å¤šä½™å¼•å·æˆ–ç©ºæ ¼ï¼Œç¡®ä¿çº¯å‡€å­—ç¬¦ä¸²
    prompt_str = str(prompt_type).strip().strip("'").strip('"')

    return f"{operation_id}_{prompt_str}"
