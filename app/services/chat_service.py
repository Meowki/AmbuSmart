import asyncio
import json
from fastapi import HTTPException, Request
from fastapi.encoders import jsonable_encoder
import openai
import os
import logging
from sqlalchemy.orm import Session
from crud.chat import create_chat_record, get_chat_history, get_operation_data, get_patient_history
from models.operation_history import OperationHistory
from openai import AsyncOpenAI, OpenAI
from utils.prompts import get_prompt_by_type

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

openai.api_key = os.getenv("DASHSCOPE_API_KEY")
openai.api_base = "https://dashscope.aliyuncs.com/compatible-mode/v1"

async def chat_with_ai(db: Session, request: Request, operation_id: int, message: str, prompt_type: str):
    operation = db.query(OperationHistory).filter(OperationHistory.operation_id == operation_id).first()
    patient_id = operation.patient_id if operation and operation.patient_id else None
    client = None
    disconnect_task = None
    stream_completed = False  # æ–°å¢æµå®Œæˆæ ‡è®°

    try:
        logger.info(f"[BACKEND] æ”¶åˆ°è¯·æ±‚: operation_id={operation_id}, prompt_type={prompt_type}")

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = AsyncOpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

        should_abort = asyncio.Event()

        # å¢å¼ºç‰ˆè¿æ¥ç›‘å¬
        async def watch_disconnect():
            logger.debug("[BACKEND] å¯åŠ¨è¿æ¥çŠ¶æ€ç›‘å¬å™¨")
            while not should_abort.is_set() and not stream_completed:
                try:
                    # å®æ—¶è¿æ¥æ£€æŸ¥
                    if await request.is_disconnected():
                        logger.warning("[BACKEND] ğŸ”´ æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥!")
                        should_abort.set()
                        break
                    await asyncio.sleep(0.3)  # 300msæ£€æµ‹é—´éš”
                except Exception as e:
                    logger.error(f"[BACKEND] ç›‘å¬å¼‚å¸¸: {str(e)}")
                    break
            logger.debug("[BACKEND] é€€å‡ºè¿æ¥ç›‘å¬")

        disconnect_task = asyncio.create_task(watch_disconnect())


        # 1. æ ¹æ®æ‚£è€…ç›®å‰æƒ…å†µæ€»ç»“å‡ºåˆæ­¥è¯Šæ–­
        # 2. æ ¹æ®å†å²è®°å½•è·å–æ€¥æ•‘å¤„ç†å’Œç”¨è¯è®°å½•
        # 3. ç”Ÿæˆæœ€ç»ˆæ€¥æ•‘ç»“æœ
        # ---> æ·»åŠ prompt key

         # è·å–å½“å‰ `operation_id` ç›¸å…³çš„å¯¹è¯å†å²å’Œæ€¥æ•‘æ•°æ®
        chat_history = get_chat_history(db, operation_id)
        operation_data = get_operation_data(db, operation_id)

        # ç»Ÿä¸€ç®¡ç† Prompt
        system_prompt = get_prompt_by_type(prompt_type) 
        logger.info(f"ä½¿ç”¨çš„prompt: operation_id={prompt_type}")


         # ç‰¹æ®Šé€»è¾‘ï¼šæ‚£è€…åŸºç¡€åˆ†æ prompt
        if prompt_type == "patient_basic_analysis" and patient_id:
            logger.info(f"operation_id {operation_id} æ˜¯æ‚£è€…åŸºç¡€åˆ†æåœºæ™¯")
            patient_data = get_patient_history(db, patient_id)

            if patient_data:
                formatted_data = json.dumps(patient_data, ensure_ascii=False)
            else:
                raise HTTPException(status_code=404, detail="æ— æ³•è·å–æ‚£è€…å†å²æ•°æ®")
        else:
            # å…¶ä»–æ‰€æœ‰åœºæ™¯
            formatted_data = f"""
            æ‚£è€…åŸºæœ¬ä¿¡æ¯ï¼š{operation_data['patient_info']}, 
            ä¸»è¯‰ï¼š{operation_data['chief_complaint']}ã€‚
            æ€¥æ•‘ç±»å‹ï¼š{operation_data['emergency_type']}ï¼Œç—…æƒ…åˆ†çº§ï¼š{operation_data['severity_level']}ã€‚
            åˆæ­¥è¯Šæ–­ï¼š{operation_data['initial_diagnosis']}ï¼Œæ€¥æ•‘å¤„ç†ï¼š{operation_data['procedures']}ï¼Œç”¨è¯ï¼š{operation_data['medicine']}ã€‚
            åˆæ£€ï¼š{operation_data['initial_check']}
            ç»ˆæ£€ï¼š{operation_data['final_check']}
            TI å†…å®¹ï¼š{operation_data['ti_content']}
            GCS è¯„åˆ†ï¼š{operation_data['gcs_score']}ï¼ŒGCS å†…å®¹ï¼š{operation_data['gcs_content']}
            Killip åˆ†çº§ï¼š{operation_data['Killip_score']}ï¼ŒKillip å†…å®¹ï¼š{operation_data['Killip_content']}ï¼ŒKillip è¯Šæ–­ï¼š{operation_data['Killip_diagnosis']}
            è„‘å’ä¸­è¯„ä¼°ï¼š{operation_data['cerebral_stroke_content']}
            """
        
         # å¦‚æœæ²¡æœ‰å†å²è®°å½•ï¼Œåˆå§‹åŒ–ä¼šè¯
        if not chat_history:
            logger.info(f"operation_id {operation_id} æ˜¯æ–°çš„ä¼šè¯ï¼Œåˆå§‹åŒ–å¯¹è¯")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "system", "content": formatted_data},  # ä¼ é€’æ‚£è€…åŠæ€¥æ•‘ç›¸å…³æ•°æ®
                {"role": "user", "content": message}  # ç¬¬ä¸€è½®ç”¨æˆ·è¾“å…¥
            ]
        else:
            # å¦‚æœæœ‰å†å²è®°å½•ï¼ŒåŠ è½½å†å²å¯¹è¯
            logger.info(f"operation_id {operation_id} è½½å…¥å†å²è®°å½•")
            messages = [{"role": "system", "content": system_prompt}]
            messages.append({"role": "system", "content": formatted_data})  # ä¼ é€’æ€¥æ•‘æ•°æ®
            for chat in chat_history:
                messages.append({"role": "user", "content": chat.user_message})
                messages.append({"role": "assistant", "content": chat.ai_response})

            # è¿½åŠ å½“å‰ç”¨æˆ·è¾“å…¥
            messages.append({"role": "user", "content": message})

        # è°ƒç”¨ AI æ¥å£
        logger.debug(f"[BACKEND] å¼€å§‹è°ƒç”¨AIæ¥å£ï¼Œprompté•¿åº¦: {len(str(messages))}")
        completion = await client.chat.completions.create(
            model="qwen-max-0125",
            messages=messages,
            stream=True,
        )

        # æµå¤„ç†
        full_response = ""
        # if operation_id in abort_requests:
        #     abort_requests.remove(operation_id)
        #     logger.warning(f"[BACKEND] è¯·æ±‚å·²è¢«æ ‡è®°ä¸ºç»ˆæ­¢ operation_id={operation_id}")
        #     yield f"data: {json.dumps({'error': 'è¯·æ±‚å·²ç»ˆæ­¢'})}\n\n"
        #     return
        
        async for chunk in completion:
            if should_abort.is_set():
                logger.warning("[BACKEND] âš ï¸ æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œä¸­æ­¢æµå¼ä¼ è¾“")
                break

            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                logger.debug(f"[BACKEND] å‘é€æ•°æ®å— size={len(content)}")
                yield f"data: {json.dumps({'response': content}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.03)

        stream_completed = True

        # æ•°æ®åº“å­˜å‚¨ (ä»…åœ¨æ­£å¸¸å®Œæˆæ—¶ä¿å­˜)
        if not should_abort.is_set():
            logger.info(f"[BACKEND] å‡†å¤‡ä¿å­˜æ•°æ®ï¼Œé•¿åº¦: {len(full_response)}")
            await save_chat_record(db, operation_id, message, full_response)
            logger.debug("[BACKEND] æ•°æ®ä¿å­˜å®Œæˆ")
        else:
            logger.warning("[BACKEND] è¯·æ±‚å·²ä¸­æ­¢ï¼Œè·³è¿‡æ•°æ®ä¿å­˜")

    except Exception as e:
        logger.error(f"[BACKEND] å¤„ç†å¼‚å¸¸: {str(e)}")
        yield f"data: {json.dumps({'error': 'å¤„ç†å¤±è´¥'})}\n\n"
    finally:
        # èµ„æºæ¸…ç†
        if disconnect_task:
            disconnect_task.cancel()
        if client:
            await client.close()
        logger.debug("[BACKEND] èµ„æºæ¸…ç†å®Œæˆ")
    
# å¼‚æ­¥ä¿å­˜è®°å½•
async def save_chat_record(db: Session, operation_id: int, message: str, response: str):
    try:
        # å‡è®¾create_chat_recordæ˜¯åŒæ­¥å‡½æ•°ï¼Œéœ€ç”¨çº¿ç¨‹æ± åŒ…è£…
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None, 
            lambda: create_chat_record(db, operation_id, message, response)
        )
    except Exception as e:
        logger.error(f"å¼‚æ­¥ä¿å­˜å¤±è´¥: {str(e)}")